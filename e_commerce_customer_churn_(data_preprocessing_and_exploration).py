# -*- coding: utf-8 -*-
"""E-Commerce Customer Churn (Data Preprocessing and Exploration)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOv4UK4sPmEDBooGerPKyogQDyuliQGk
"""

# -*- coding: utf-8 -*-
"""
Ilfa Antaries Yusuf Wijaya | E-commerce Customer Churn Data Preprocessing and Exploration
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from IPython.display import display
import os
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)

def load_data(file_path: str) -> pd.DataFrame:
    """
    Load the dataset from a specified file path with error handling.

    Parameters:
        file_path (str): The path to the CSV file.

    Returns:
        pd.DataFrame: The loaded dataset as a Pandas DataFrame.

    Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If there is an error reading the file.
    """
    if not os.path.exists(file_path):
        logging.error(f"File not found: {file_path}")
        raise FileNotFoundError(f"File not found: {file_path}")

    try:
        df = pd.read_csv(file_path)
        logging.info(f"Loaded data from {file_path}")
        return df
    except pd.errors.EmptyDataError:
        logging.error("No data: The file is empty.")
        raise ValueError("No data: The file is empty.")
    except pd.errors.ParserError:
        logging.error("Error parsing the file: Ensure it is a valid CSV.")
        raise ValueError("Error parsing the file: Ensure it is a valid CSV.")
    except Exception as e:
        logging.error(f"Error reading the file: {e}")
        raise ValueError(f"Error reading the file: {e}")

def initial_data_check(df: pd.DataFrame) -> None:
    """
    Display the first five rows, dataset info, and a summary of numeric columns.

    Parameters:
        df (pd.DataFrame): The dataset to check.
    """
    logging.info("Performing initial data check...")
    print("\nFirst 5 rows of the dataset:\n")
    display(df.head())

    print("\nDataset Information:\n")
    df.info()

    print("\nSummary Statistics for Numeric Columns:\n")
    display(df.describe())

def clean_data(df: pd.DataFrame, cols_to_fill: list) -> pd.DataFrame:
    """
    Clean the dataset by removing duplicates and handling missing values.

    Parameters:
        df (pd.DataFrame): The dataset to clean.
        cols_to_fill (list): List of column names to fill missing values.

    Returns:
        pd.DataFrame: The cleaned dataset.
    """
    logging.info("Cleaning data...")

    # Remove duplicate rows
    duplicates_count = df.duplicated().sum()
    print(f"\nNumber of duplicate rows: {duplicates_count}")
    if duplicates_count > 0:
        df.drop_duplicates(inplace=True)

    # Handle missing values by filling with the median for specific columns
    print("\nMissing values count per column before handling:\n")
    display(df.isna().sum())

    for col in cols_to_fill:
        if col in df.columns:
            df[col].fillna(df[col].median(), inplace=True)

    # Verify that missing values have been handled
    print("\nMissing values count per column after handling:\n")
    display(df.isna().sum())

    return df

def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transform the dataset by creating new categorical columns.

    Parameters:
        df (pd.DataFrame): The dataset to transform.

    Returns:
        pd.DataFrame: The transformed dataset.
    """
    logging.info("Transforming data...")

    # Create a 'TenureGroup' column based on 'Tenure'
    if 'Tenure' in df.columns:
        df['TenureGroup'] = pd.cut(
            df['Tenure'], bins=[0, 12, 36, np.inf], labels=['Baru', 'Sedang', 'Lama']
        )

    # Create a 'CustomerValue' column based on 'CashbackAmount'
    if 'CashbackAmount' in df.columns:
        df['CustomerValue'] = pd.cut(
            df['CashbackAmount'], bins=[0, 200, 500, np.inf], labels=['Low', 'Medium', 'High']
        )

    print("\nFirst 5 rows after adding new features:\n")
    display(df.head())
    return df

def plot_numerical_distributions(df: pd.DataFrame, columns: list) -> None:
    """
    Plot distributions for numerical features using histograms with KDE.

    Parameters:
        df (pd.DataFrame): The dataset.
        columns (list): List of numerical column names to plot.
    """
    logging.info("Plotting numerical distributions...")
    plt.figure(figsize=(12, 12))
    for i, col in enumerate(columns, 1):
        plt.subplot(4, 2, i)
        sns.histplot(df[col], kde=True)
        plt.title(f'Distribusi {col}')
    plt.tight_layout()
    plt.show()

def plot_categorical_counts(df: pd.DataFrame, columns: list) -> None:
    """
    Plot count plots for categorical features.

    Parameters:
        df (pd.DataFrame): The dataset.
        columns (list): List of categorical column names to plot.
    """
    logging.info("Plotting categorical counts...")
    plt.figure(figsize=(12, 8))
    for i, col in enumerate(columns, 1):
        plt.subplot(2, 2, i)
        sns.countplot(data=df, x=col, palette='viridis')
        plt.title(f'Count of {col}')
        plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def plot_correlation_heatmap(df: pd.DataFrame) -> None:
    """
    Plot a heatmap to visualize the correlation between numeric features.

    Parameters:
        df (pd.DataFrame): The dataset.
    """
    logging.info("Plotting correlation heatmap...")
    corr = df.select_dtypes(include=['float64', 'int64']).corr()
    plt.figure(figsize=(10, 6))
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Heatmap Korelasi')
    plt.show()

def save_processed_data(df: pd.DataFrame, file_path: str) -> None:
    """
    Save the processed dataset to a CSV file.

    Parameters:
        df (pd.DataFrame): The dataset to save.
        file_path (str): The path where the file will be saved.
    """
    df.to_csv(file_path, index=False)
    logging.info(f"Dataset successfully saved at: {file_path}")

# Main Execution Flow
if __name__ == "__main__":
    # Define dataset path
    dataset_path = '/content/data_ecommerce_customer_churn.csv'
    processed_dataset_path = '/content/processed_ecommerce_customer_churn.csv'

    # Load and explore the dataset
    try:
        df = load_data(dataset_path)
        initial_data_check(df)

        # Clean and transform the dataset
        df = clean_data(df, ['Tenure', 'WarehouseToHome', 'DaySinceLastOrder'])
        df = transform_data(df)

        # Define columns for visualization
        numerical_cols = ['Tenure', 'WarehouseToHome', 'NumberOfDeviceRegistered',
                          'SatisfactionScore', 'NumberOfAddress',
                          'DaySinceLastOrder', 'CashbackAmount']
        categorical_cols = ['PreferedOrderCat', 'MaritalStatus', 'Churn', 'TenureGroup']

        # Plot visualizations
        plot_numerical_distributions(df, numerical_cols)
        plot_categorical_counts(df, categorical_cols)
        plot_correlation_heatmap(df)

        # Save the cleaned and processed dataset
        save_processed_data(df, processed_dataset_path)

    except Exception as e:
        logging.error(f"An error occurred: {e}")